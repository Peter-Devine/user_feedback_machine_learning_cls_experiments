{"ciurumelea_2017": {"test_loss": 0.5383674502372742, "test_accuracy": 0.7635782747603834, "test_f1": 0.6837606837606838, "test_precision": 0.6201550387596899, "test_recall": 0.7619047619047619, "test_roc_auc": 0.8449175824175825, "test_runtime": 0.407, "test_samples_per_second": 768.97, "test_steps_per_second": 12.284}, "guzman_2015": {"test_loss": 0.6148777604103088, "test_accuracy": 0.734321550741163, "test_f1": 0.6148760330578512, "test_precision": 0.45925925925925926, "test_recall": 0.93, "test_roc_auc": 0.8742023633677991, "test_runtime": 2.0556, "test_samples_per_second": 426.635, "test_steps_per_second": 6.811}, "scalabrino_2017_RQ1": {"test_loss": 0.5532726645469666, "test_accuracy": 0.7525083612040134, "test_f1": 0.6372549019607844, "test_precision": 0.5078125, "test_recall": 0.8552631578947368, "test_roc_auc": 0.8770208874203446, "test_runtime": 0.926, "test_samples_per_second": 645.779, "test_steps_per_second": 10.799}, "scalabrino_2017_RQ3": {"test_loss": 0.5124521851539612, "test_accuracy": 0.7605633802816901, "test_f1": 0.8229166666666666, "test_precision": 0.8494623655913979, "test_recall": 0.797979797979798, "test_roc_auc": 0.8376791167488842, "test_runtime": 0.258, "test_samples_per_second": 550.41, "test_steps_per_second": 11.628}, "maalej_2016": {"test_loss": 0.7281901240348816, "test_accuracy": 0.6875, "test_f1": 0.3382352941176471, "test_precision": 0.23, "test_recall": 0.6388888888888888, "test_roc_auc": 0.7431106701940036, "test_runtime": 0.6823, "test_samples_per_second": 422.09, "test_steps_per_second": 7.328}, "tizard_2019": {"test_loss": 0.7492542862892151, "test_accuracy": 0.6639118457300276, "test_f1": 0.21290322580645157, "test_precision": 0.13580246913580246, "test_recall": 0.4925373134328358, "test_roc_auc": 0.6939052838991688, "test_runtime": 1.2535, "test_samples_per_second": 579.185, "test_steps_per_second": 9.573}}